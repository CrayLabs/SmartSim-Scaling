import pandas as pd
import os.path as osp
from itertools import product


from smartsim import Experiment
from smartsim.settings import SrunSettings, SbatchSettings
from smartsim.database import SlurmOrchestrator
from smartsim import slurm
from silc import Client
from smartsim import constants
"""Run an experiment for testing the scaling of the SILC c++ client.
   This script will run as many permutations of CLIENT_NODES and
   CPN (Clients per node) listed in the globals at the top.

   The post processing script collects the statistics generated by
   the inference workloads and combines them into a final CSV saved
   under NAME.csv
"""

# Constants
NAME = "infer-resnet"        # name of experiment directory
DB_NODES = 16                 # number of database nodes
DPN = 1                      # number of databases per node
CLIENT_ALLOC = 100           # number of nodes in client alloc
CLIENT_NODES = [20, 40, 60, 80, 100]      # list of node sizes to run clients within client alloc
CPN = [80]                   # clients per node
DB_CPUS = 36                 # number of CPUS per database shard
DB_TPQ = 4                   # database threads per queue
BATCH_SIZE = 10              # batch size of DL model
DEVICE = "GPU"               # device for script and model
MODEL = "../imagenet/resnet-1.5.0.pt" # path to DL model


# obtain allocation for client
client_add_opts = {
    "ntasks-per-node": max(CPN),
    "time": "1:00:00",
    "exclusive": None
}
client_alloc = slurm.get_slurm_allocation(nodes=CLIENT_ALLOC, add_opts=client_add_opts)


exp = Experiment(NAME, launcher="slurm")

def setup_pytorch(device, batch_size):
    """Set Pytorch model and script inside database"""
    
    client = Client(address="10.128.0.154:6780", cluster=bool(DB_NODES>1))
    client.set_model_from_file("resnet_model", MODEL, "TORCH", device, batch_size)
    client.set_script_from_file("resnet_script", "../imagenet/data_processing_script.txt", device)

def setup_run(nodes, tasks):
    """Construct an MPI program with one SILC client on each rank
    to perform parallel inference.
    """
    run_args = {
        "nodes": nodes,
        "ntasks-per-node": tasks
    }
    srun = SrunSettings("./build/run_resnet_inference", run_args=run_args, alloc=client_alloc)

    name = "-".join(("infer-sess", str(nodes), str(tasks)))
    model = exp.create_model(name, srun)
    model.attach_generator_files(to_copy=["../imagenet/cat.raw", "./process_results.py"])
    exp.generate(model, overwrite=True)
    return model

def start_database():
    """Create a empty database for each run b/c resnet images take up alot of space"""

    db = SlurmOrchestrator(port=6780, db_nodes=DB_NODES, batch=True, threads_per_queue=DB_TPQ)
    db.set_cpus(DB_CPUS)
    db.set_walltime("10:00:00")
    db.batch_settings.batch_args["exclusive"] = None
    db.batch_settings.batch_args["C"] = "P100"
    exp.generate(db)
    exp.start(db)
    return db

def setup_post_process(model_path, name):
    """Post Process the results of the inference session
    for analysis.
    """

    exe_args = f"process_results.py --path={model_path} --name={name}"
    srun = SrunSettings("python", exe_args=exe_args, alloc=client_alloc)
    srun.set_nodes(1)
    srun.set_tasks(1)

    pp_name = "-".join(("post", name))
    post_process = exp.create_model(pp_name, srun, path=model_path)
    return post_process


def get_stats(data_locations):
    """Obtain inference statistics"""

    all_data = None
    for data_path, name, job_info in data_locations:
        data_path = osp.join(data_path, name + ".csv")
        data = pd.read_csv(data_path)
        data = data.drop("Unnamed: 0", axis=1)

        # add node and task information
        data["nodes"] = job_info[0]
        data["tasks"] = job_info[1]

        if not isinstance(all_data, pd.DataFrame):
            all_data = data
        else:
            all_data = pd.concat([all_data, data])
    return all_data


# run all permutations of nodes and clients per node listed
perms = list(product(CLIENT_NODES, CPN))
data_locations = []
for perm in perms:
    # start a new database
    db = start_database()
    print("Database setup")
    # set models and script
    setup_pytorch(DEVICE, BATCH_SIZE)
    print("Model and script setup")
    # setup a an instance of the C++ driver and start it
    infer_session = setup_run(*perm)
    exp.start(infer_session, summary=True)
    status = exp.get_status(infer_session)
    if status[0] != constants.STATUS_COMPLETED:
        print(f"ERROR: One of the simulations failed {infer_session.name}")
        break

    # get the statistics from the run
    post = setup_post_process(infer_session.path, infer_session.name)
    data_locations.append((infer_session.path, infer_session.name, perm))
    exp.start(post)

    exp.stop(db)

try:
    # get the statistics from post processing
    # and add to the experiment summary
    stats_df = get_stats(data_locations)
    summary_df = exp.summary()
    final_df = pd.merge(summary_df, stats_df, on="Name")

    # save experiment info
    print(final_df)
    final_df.to_csv(NAME + ".csv")

except Exception:
    print("Could not preprocess results")

slurm.release_slurm_allocation(client_alloc)
